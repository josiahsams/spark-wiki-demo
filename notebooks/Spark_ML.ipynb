{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "### Classify a subset of wikipedia articles into 100 different Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans Algorithm\n",
    "[image1]: ./images/kmeans.jpg\n",
    "![pipeline][image1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val ss = SparkSession.builder().getOrCreate()\n",
    "import ss.implicits._\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n",
       "wikiDF: org.apache.spark.sql.DataFrame = [id: bigint, title: string ... 7 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val wikiDF = spark.read.parquet(\"/Users/josiahsams/SparkDemo/datasets/wiki.parquet\")\n",
    "            .filter($\"text\".isNotNull)\n",
    "            .select($\"id\", $\"title\", $\"revid\", \n",
    "                    $\"lastrev_pdt_time\", $\"comment\", \n",
    "                    $\"contributorid\", $\"contributorusername\",\n",
    "                    $\"contributorip\", lower($\"text\").as(\"lowerText\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- revid: long (nullable = true)\n",
      " |-- lastrev_pdt_time: timestamp (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      " |-- contributorid: long (nullable = true)\n",
      " |-- contributorusername: string (nullable = true)\n",
      " |-- contributorip: string (nullable = true)\n",
      " |-- lowerText: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wikiDF.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset contains Articles updated in the last 3 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res84: Long = 3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wikiDF.select(dayofyear($\"lastrev_pdt_time\")).distinct().count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     dates|\n",
      "+----------+\n",
      "|2017-09-30|\n",
      "|2017-10-01|\n",
      "|2017-10-02|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wikiDF.select(to_date($\"lastrev_pdt_time\").as(\"dates\")).orderBy($\"dates\").distinct.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.86%\n"
     ]
    }
   ],
   "source": [
    "printf(\"%.2f%%\\n\", wikiDF.count()/5096292.0*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               title|    lastrev_pdt_time|\n",
      "+--------------------+--------------------+\n",
      "|Fingal (music group)|2017-10-01 06:10:...|\n",
      "|File:KXOL-AM logo...|2017-10-01 07:55:...|\n",
      "|Komorze Nowomiejskie|2017-09-30 23:40:...|\n",
      "|Adamowo, Wolsztyn...|2017-09-30 21:36:...|\n",
      "|Stara Dąbrowa, Wo...|2017-09-30 21:42:...|\n",
      "|               TopoR|2017-10-01 23:49:...|\n",
      "|Jeremy Bates (Ame...|2017-10-01 20:58:...|\n",
      "|       Matthew Mixer|2017-09-30 19:53:...|\n",
      "|     Western culture|2017-09-30 17:43:...|\n",
      "|History of FC Bar...|2017-10-01 12:18:...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wikiDF.select($\"title\", $\"lastrev_pdt_time\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikiDF.createOrReplaceTempView(\"wikipedia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Articles edited by Bots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res163: org.apache.toree.magic.MagicOutput =\n",
       "MagicOutput(ArrayBuffer((text/plain,+--------+\n",
       "|count(1)|\n",
       "+--------+\n",
       "|     516|\n",
       "+--------+\n",
       ")))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%sql\n",
    "select count(*) from wikipedia where contributorusername like \"%Bot %\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who is the most active contributor in the last 3 days ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res164: org.apache.toree.magic.MagicOutput =\n",
       "MagicOutput(ArrayBuffer((text/plain,+-------------------+--------------------------+\n",
       "|contributorusername|count(contributorusername)|\n",
       "+-------------------+--------------------------+\n",
       "|       Red Director|                      4003|\n",
       "| InternetArchiveBot|                      3134|\n",
       "|             DatBot|                      1970|\n",
       "|          WOSlinker|                      1716|\n",
       "|            AvicBot|                      1077|\n",
       "|             Dawynn|                       992|\n",
       "|         WP 1.0 bot|                       886|\n",
       "|               Tim!|                       879|\n",
       "|             Hmains|                       846|\n",
       "|           Onel5969|                       818|\n",
       "+-------------------+--------------------------+\n",
       "only showing top 10 rows\n",
       ")))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%SQL\n",
    "select  contributorusername, count(contributorusername) from wikipedia group by contributorusername order by count(contributorusername) desc  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task for a Data Scientist "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark String Tokenizer using Regular Expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformers RegexTokenizer : Transform a dataframe into another dataframe with additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_9e08f2608e68\n",
       "wikiWordsDF: org.apache.spark.sql.DataFrame = [id: bigint, title: string ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.RegexTokenizer\n",
    " \n",
    "val tokenizer = new RegexTokenizer()\n",
    "  .setInputCol(\"lowerText\")\n",
    "  .setOutputCol(\"words\")\n",
    "  .setPattern(\"\\\\W+\")\n",
    "\n",
    "val wikiWordsDF = tokenizer.transform(wikiDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               title|               words|\n",
      "+--------------------+--------------------+\n",
      "|Fingal (music group)|[image, with, unk...|\n",
      "|File:KXOL-AM logo...|[summary, logo, f...|\n",
      "|Komorze Nowomiejskie|[infobox, settlem...|\n",
      "|Adamowo, Wolsztyn...|[otherplaces, ada...|\n",
      "|Stara Dąbrowa, Wo...|[other, places, s...|\n",
      "|               TopoR|[redir, toporoute...|\n",
      "|Jeremy Bates (Ame...|[infobox, college...|\n",
      "|       Matthew Mixer|[no, footnotes, b...|\n",
      "|     Western culture|[about, this, art...|\n",
      "|History of FC Bar...|[about, a, statis...|\n",
      "|John Archer (poli...|[other, people, j...|\n",
      "|Tremont Avenue–17...|[other, uses, tre...|\n",
      "|Wikipedia:WikiPro...|[anchor, aastart,...|\n",
      "|Wikipedia:Article...|[div, class, boil...|\n",
      "|List of Soviet fi...|[soviet, film, li...|\n",
      "|           L. Gordon|[no, footnotes, d...|\n",
      "|              Raygun|[about, the, fict...|\n",
      "|Black and White (...|[unreferenced, da...|\n",
      "|        Chabrouh Dam|[the, faraya, cha...|\n",
      "|Wikipedia:Article...|[div, class, boil...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wikiWordsDF.select($\"title\", $\"words\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common words in the edited articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tenPercentDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: bigint, title: string ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val tenPercentDF = wikiWordsDF.sample(false, .01, 555).cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94,924 words (total)\n",
      "994 words (sample)\n"
     ]
    }
   ],
   "source": [
    "printf(\"%,d words (total)\\n\", wikiWordsDF.count)\n",
    "printf(\"%,d words (sample)\\n\", tenPercentDF.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explode the words column into a table of one word per row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tenPercentWordsListDF: org.apache.spark.sql.DataFrame = [word: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val tenPercentWordsListDF = tenPercentDF.select(explode($\"words\").as(\"word\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|        word|\n",
      "+------------+\n",
      "|     infobox|\n",
      "|officeholder|\n",
      "|        name|\n",
      "|     filippo|\n",
      "|   tamagnini|\n",
      "|      office|\n",
      "|        list|\n",
      "|          of|\n",
      "|    captains|\n",
      "|      regent|\n",
      "|          of|\n",
      "|         san|\n",
      "|      marino|\n",
      "|     captain|\n",
      "|      regent|\n",
      "|          of|\n",
      "|         san|\n",
      "|      marino|\n",
      "|   alongside|\n",
      "|       maria|\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tenPercentWordsListDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,743,246 words\n"
     ]
    }
   ],
   "source": [
    "printf(\"%,d words\\n\", tenPercentWordsListDF.cache().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordGroupCountDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [word: string, counts: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| word|counts|\n",
      "+-----+------+\n",
      "|  the| 88288|\n",
      "|   of| 48834|\n",
      "|  ref| 38809|\n",
      "|  and| 37230|\n",
      "|   in| 36719|\n",
      "|    a| 27794|\n",
      "|   to| 27402|\n",
      "|    s| 17555|\n",
      "|title| 17407|\n",
      "| http| 15277|\n",
      "|  for| 14738|\n",
      "|  was| 14436|\n",
      "|   by| 14144|\n",
      "|   on| 13677|\n",
      "| name| 13097|\n",
      "+-----+------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val wordGroupCountDF = tenPercentWordsListDF\n",
    "                      .groupBy(\"word\")  // group\n",
    "                      .agg(count(\"word\").as(\"counts\"))  // aggregate\n",
    "                      .sort(desc(\"counts\"))  // sort\n",
    "\n",
    "wordGroupCountDF.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res156: tenPercentWordsListDF.type = [word: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tenPercentWordsListDF.unpersist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good to clean up the \"stop words\" before running Natural Language Processing algorithms on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stopwords: Array[String] = Array(a, about, above, after, again, against, all, am, an, and, any, are, aren't, as, at, be, because, been, before, being, below, between, both, but, by, can't, cannot, could, couldn't, did, didn't, do, does, doesn't, doing, don't, down, during, each, few, for, from, further, had, hadn't, has, hasn't, have, haven't, having, he, he'd, he'll, he's, her, here, here's, hers, herself, him, himself, his, how, how's, i, i'd, i'll, i'm, i've, if, in, into, is, isn't, it, it's, its, itself, let's, me, more, most, mustn't, my, myself, no, nor, not, of, off, on, once, only, or, other, ought, our, ours, ourselves, out, over, own, same, shan't, she, she'd, she'll, she's, should, shouldn't, s, so, some, such, than, that, ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "\n",
    "val stopwords = spark.read.textFile(\"/Users/josiahsams/SparkDemo/datasets/stopwords.txt\").collect\n",
    "\n",
    "val remover = new StopWordsRemover()\n",
    "  .setInputCol(\"words\")\n",
    "  .setOutputCol(\"noStopWords\")\n",
    "  .setStopWords(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+\n",
      "|      id|               title|               words|         noStopWords|\n",
      "+--------+--------------------+--------------------+--------------------+\n",
      "|31361855|   Filippo Tamagnini|[infobox, officeh...|[infobox, officeh...|\n",
      "| 2216434|         FK Pelister|[infobox, footbal...|[infobox, footbal...|\n",
      "| 9772924|      Trevor Hebberd|[engvarb, date, j...|[engvarb, july, u...|\n",
      "| 4674998|Borodino-class ba...|[about, the, boro...|[borodino, battle...|\n",
      "|53866204|Template:2017–18 ...|[noinclude, read,...|[noinclude, read,...|\n",
      "|53871267|               Saaho|[use, indian, eng...|[use, indian, eng...|\n",
      "|53980626|      Vicente Parras|[infobox, footbal...|[infobox, footbal...|\n",
      "|54142804|Recurring Saturda...|[unreferenced, da...|[unreferenced, ma...|\n",
      "| 6068811|Category:Zoos in ...|[commons, categor...|[commons, zoos, w...|\n",
      "| 2560194|    Brian Culbertson|[use, mdy, dates,...|[use, mdy, dates,...|\n",
      "|  229723|                 Lie|[other, uses, red...|[uses, redirect, ...|\n",
      "|37521110|County Bridge No....|[infobox, nrhp, n...|[infobox, nrhp, c...|\n",
      "|49888670|Wikipedia:WikiPro...|[wikidata, list, ...|[wikidata, list, ...|\n",
      "|55100117|File:Sex and the ...|[orphaned, non, f...|[orphaned, non, f...|\n",
      "|55248327|File:Bank of Engl...|[summary, non, fr...|[summary, non, fr...|\n",
      "+--------+--------------------+--------------------+--------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remover.transform(tenPercentDF).select(\"id\", \"title\", \"words\", \"noStopWords\").show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "noStopWordsListDF: org.apache.spark.sql.DataFrame = [word: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val noStopWordsListDF = remover.transform(tenPercentDF).select(explode($\"noStopWords\").as(\"word\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|        word|\n",
      "+------------+\n",
      "|     infobox|\n",
      "|officeholder|\n",
      "|     filippo|\n",
      "|   tamagnini|\n",
      "|      office|\n",
      "|        list|\n",
      "|    captains|\n",
      "|      regent|\n",
      "|         san|\n",
      "|      marino|\n",
      "+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "noStopWordsListDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,711,368 words\n"
     ]
    }
   ],
   "source": [
    "printf(\"%,d words\\n\", noStopWordsListDF.cache.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "noStopWordsGroupCount: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [word: string, counts: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|     word|counts|\n",
      "+---------+------+\n",
      "|september|  3655|\n",
      "|      may|  3630|\n",
      "|   united|  3490|\n",
      "|   closed|  3392|\n",
      "|   august|  3220|\n",
      "|       uk|  3171|\n",
      "|     year|  3165|\n",
      "|      one|  3160|\n",
      "|     list|  3087|\n",
      "|     left|  2900|\n",
      "| national|  2887|\n",
      "|       17|  2881|\n",
      "| articles|  2875|\n",
      "|    right|  2856|\n",
      "|  october|  2831|\n",
      "|     june|  2814|\n",
      "|       18|  2799|\n",
      "|     city|  2792|\n",
      "|     2008|  2779|\n",
      "|     july|  2757|\n",
      "|       15|  2740|\n",
      "|  january|  2720|\n",
      "|   states|  2689|\n",
      "|       16|  2685|\n",
      "|   series|  2673|\n",
      "|     film|  2652|\n",
      "|   season|  2636|\n",
      "|    world|  2626|\n",
      "|       14|  2573|\n",
      "|       13|  2548|\n",
      "+---------+------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val noStopWordsGroupCount = noStopWordsListDF\n",
    "                      .groupBy(\"word\")  // group\n",
    "                      .agg(count(\"word\").as(\"counts\"))  // aggregate\n",
    "                      .sort(desc(\"counts\"))  // sort\n",
    "\n",
    "noStopWordsGroupCount.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138,945 distinct words"
     ]
    }
   ],
   "source": [
    "printf(\"%,d distinct words\", noStopWordsListDF.distinct.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.{RegexTokenizer, StopWordsRemover, HashingTF, IDF, Normalizer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "noStopWordsListDF: org.apache.spark.sql.DataFrame = [id: bigint, title: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val noStopWordsListDF = remover.transform(wikiWordsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+\n",
      "|      id|               title|               words|         noStopWords|\n",
      "+--------+--------------------+--------------------+--------------------+\n",
      "|21127403|Fingal (music group)|[image, with, unk...|[image, unknown, ...|\n",
      "|21145234|File:KXOL-AM logo...|[summary, logo, f...|[summary, logo, f...|\n",
      "|21157758|Komorze Nowomiejskie|[infobox, settlem...|[infobox, settlem...|\n",
      "|21158912|Adamowo, Wolsztyn...|[otherplaces, ada...|[otherplaces, ada...|\n",
      "|21158944|Stara Dąbrowa, Wo...|[other, places, s...|[places, stara, b...|\n",
      "|21174956|               TopoR|[redir, toporoute...|[redir, toporoute...|\n",
      "|21188846|Jeremy Bates (Ame...|[infobox, college...|[infobox, college...|\n",
      "|21197084|       Matthew Mixer|[no, footnotes, b...|[footnotes, blp, ...|\n",
      "|21208262|     Western culture|[about, this, art...|[article, equival...|\n",
      "|21221470|History of FC Bar...|[about, a, statis...|[statistical, bre...|\n",
      "|21233027|John Archer (poli...|[other, people, j...|[people, john, ar...|\n",
      "|21246898|Tremont Avenue–17...|[other, uses, tre...|[uses, tremont, a...|\n",
      "|21257162|Wikipedia:WikiPro...|[anchor, aastart,...|[anchor, aastart,...|\n",
      "|21263313|Wikipedia:Article...|[div, class, boil...|[div, class, boil...|\n",
      "|21276625|List of Soviet fi...|[soviet, film, li...|[soviet, film, li...|\n",
      "+--------+--------------------+--------------------+--------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "noStopWordsListDF.select(\"id\", \"title\", \"words\", \"noStopWords\").show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hashingTF: org.apache.spark.ml.feature.HashingTF = hashingTF_d7631946c6e2\n",
       "featurizedDataDF: org.apache.spark.sql.DataFrame = [id: bigint, title: string ... 11 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// More features = more complexity and computational time and accuracy\n",
    "\n",
    "val hashingTF = new HashingTF().setInputCol(\"noStopWords\").setOutputCol(\"hashingTF\").setNumFeatures(20000)\n",
    "val featurizedDataDF = hashingTF.transform(noStopWordsListDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+\n",
      "|      id|               title|         noStopWords|           hashingTF|\n",
      "+--------+--------------------+--------------------+--------------------+\n",
      "|21127403|Fingal (music group)|[image, unknown, ...|(20000,[196,230,3...|\n",
      "|21145234|File:KXOL-AM logo...|[summary, logo, f...|(20000,[32,278,34...|\n",
      "|21157758|Komorze Nowomiejskie|[infobox, settlem...|(20000,[15,589,83...|\n",
      "|21158912|Adamowo, Wolsztyn...|[otherplaces, ada...|(20000,[15,212,57...|\n",
      "|21158944|Stara Dąbrowa, Wo...|[places, stara, b...|(20000,[15,212,65...|\n",
      "|21174956|               TopoR|[redir, toporoute...|(20000,[15,20,22,...|\n",
      "|21188846|Jeremy Bates (Ame...|[infobox, college...|(20000,[1,15,20,8...|\n",
      "+--------+--------------------+--------------------+--------------------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurizedDataDF.select(\"id\", \"title\", \"noStopWords\", \"hashingTF\").show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimator -  IDF : \n",
    "\n",
    "* Transform a dataframe into Model\n",
    "* This model is a transformer which can transform a dataframe into another dataframe with additional columns (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idf: org.apache.spark.ml.feature.IDF = idf_3314bdc1993f\n",
       "idfModel: org.apache.spark.ml.feature.IDFModel = idf_3314bdc1993f\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// This will take 3 - 4 mins to run\n",
    "val idf = new IDF().setInputCol(\"hashingTF\").setOutputCol(\"idf\")\n",
    "val idfModel = idf.fit(featurizedDataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normalizer: org.apache.spark.ml.feature.Normalizer = normalizer_93cac342c469\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// A normalizer is a common operation for text classification.\n",
    "\n",
    "// It simply gets all of the data on the same scale... for example, if one article is much longer and another, it'll normalize the scales for the different features.\n",
    "\n",
    "// If we don't normalize, an article with more words would be weighted differently\n",
    "\n",
    "val normalizer = new Normalizer()\n",
    "  .setInputCol(\"idf\")\n",
    "  .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Spark ML pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[image2]: ./images/ml-pipeline1.png\n",
    "![mlpipeline][image2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// This will take over 1 hour to run!\n",
    "\n",
    "/*\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.clustering.KMeans\n",
    " \n",
    "val kmeans = new KMeans()\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setK(100)\n",
    "  .setSeed(0) // for reproducability\n",
    " \n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(tokenizer, remover, hashingTF, idf, normalizer, kmeans))  \n",
    " \n",
    "val model = pipeline.fit(wikiDF)\n",
    "\n",
    "model.save(\"/Users/josiahsams/SparkDemo/datasets/wiki.model\")\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types will not be printed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n",
       "model2: org.apache.spark.ml.PipelineModel = pipeline_fc837089157a\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val model2 = org.apache.spark.ml.PipelineModel.load(\"/Users/josiahsams/SparkDemo/datasets/wiki.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawPredictionsDF: org.apache.spark.sql.DataFrame = [id: bigint, title: string ... 13 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val rawPredictionsDF = model2.transform(wikiDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res147: Array[String] = Array(id, title, revid, lastrev_pdt_time, comment, contributorid, contributorusername, contributorip, lowerText, words, noStopWords, hashingTF, idf, features, prediction)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rawPredictionsDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionsDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [title: string, prediction: int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val predictionsDF = rawPredictionsDF.select($\"title\", $\"prediction\").cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output will NOT be truncated\n",
      "+--------------------+----------+\n",
      "|               title|prediction|\n",
      "+--------------------+----------+\n",
      "|Fingal (music group)|        29|\n",
      "|File:KXOL-AM logo...|         6|\n",
      "|Komorze Nowomiejskie|        37|\n",
      "|Adamowo, Wolsztyn...|        37|\n",
      "|Stara Dąbrowa, Wo...|        37|\n",
      "|               TopoR|        87|\n",
      "|Jeremy Bates (Ame...|        46|\n",
      "|       Matthew Mixer|        15|\n",
      "|     Western culture|        81|\n",
      "|History of FC Bar...|        86|\n",
      "+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionsDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|        29|21923|\n",
      "|        81| 9552|\n",
      "|        12| 6886|\n",
      "|        87| 4714|\n",
      "|        21| 3961|\n",
      "|        42| 3904|\n",
      "|        58| 2948|\n",
      "|        86| 2641|\n",
      "|         6| 2423|\n",
      "|        91| 1980|\n",
      "+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// This will take 4-5 minutes\n",
    "predictionsDF.groupBy(\"prediction\").count().orderBy($\"count\" desc).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               title|prediction|\n",
      "+--------------------+----------+\n",
      "|Filmfare Award fo...|        27|\n",
      "|Filmfare Award fo...|        27|\n",
      "|    2011 Emmy Awards|        27|\n",
      "|        IAWTV Awards|        27|\n",
      "|Sumathi Best Tele...|        27|\n",
      "|          Saba Qamar|        27|\n",
      "|List of awards an...|        27|\n",
      "|List of awards an...|        27|\n",
      "|List of awards an...|        27|\n",
      "| World Travel Awards|        27|\n",
      "+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Awards\n",
    "predictionsDF.filter(\"prediction = 27\").select(\"title\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               title|prediction|\n",
      "+--------------------+----------+\n",
      "|    HMS Alban (1806)|        11|\n",
      "|TSS Duke of Lanca...|        11|\n",
      "|Borodino-class ba...|        11|\n",
      "|USS Glacier (AK-183)|        11|\n",
      "|USS Muscatine (AK...|        11|\n",
      "|USS Beaufort (PCS...|        11|\n",
      "|USNS Private John...|        11|\n",
      "| Orpheus (1818 ship)|        11|\n",
      "|USS Dukes County ...|        11|\n",
      "|USS Alligator (1862)|        11|\n",
      "+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Navy war ships\n",
    "predictionsDF.filter(\"prediction = 11\").select(\"title\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               title|prediction|\n",
      "+--------------------+----------+\n",
      "|Samsung Galaxy A3...|        57|\n",
      "|   Samsung Galaxy S4|        57|\n",
      "|  Samsung Experience|        57|\n",
      "|Samsung Internet ...|        57|\n",
      "|Renault Samsung M...|        57|\n",
      "|Samsung Galaxy Ta...|        57|\n",
      "|File:Logo of Sams...|        57|\n",
      "|    Japanese noctule|        57|\n",
      "|Samsung Galaxy S III|        57|\n",
      "|   Samsung Galaxy J5|        57|\n",
      "+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Samsung phones\n",
    "predictionsDF.filter(\"prediction = 57\").select(\"title\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               title|prediction|\n",
      "+--------------------+----------+\n",
      "|Columbia Regional...|        56|\n",
      "|List of airports ...|        56|\n",
      "|Gregorio Luperón ...|        56|\n",
      "|New Plymouth Airport|        56|\n",
      "|Goa International...|        56|\n",
      "|Cologne Bonn Airport|        56|\n",
      "|            Sita Air|        56|\n",
      "|Yemelyanovo Inter...|        56|\n",
      "|List of airports ...|        56|\n",
      "|Dallas/Fort Worth...|        56|\n",
      "+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// airports airlines\n",
    "predictionsDF.filter(\"prediction = 56\").select(\"title\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
