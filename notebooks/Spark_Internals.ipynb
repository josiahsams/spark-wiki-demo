{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Data Engineer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to keep the Spark UI run in port no. http://localhost:4040/jobs open in a browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load/Read CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "columns: Seq[String] = List(timestamp, site, requests)\n",
       "df: org.apache.spark.sql.DataFrame = [timestamp: timestamp, site: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val columns = Seq(\"timestamp\", \"site\", \"requests\")\n",
    "val df = spark.read.option(\"inferSchema\", true)\n",
    "    .option(\"header\", true)\n",
    "    .csv(\"/Users/josiahsams/SparkDemo/datasets/pageviews-by-second-tsv.csv\")\n",
    "    .toDF(columns: _*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to use inferSchema specially when dealing with csv to let Spark infer the Schema for us by sampling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- site: string (nullable = true)\n",
      " |-- requests: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------+\n",
      "|           timestamp|   site|requests|\n",
      "+--------------------+-------+--------+\n",
      "|2015-03-16 01:30:...|desktop|    2288|\n",
      "|2015-03-16 01:57:...| mobile|    1825|\n",
      "|2015-03-16 02:20:...|desktop|    2350|\n",
      "|2015-03-16 03:06:...|desktop|    2476|\n",
      "|2015-03-16 03:52:...|desktop|    2283|\n",
      "|2015-03-16 04:28:...|desktop|    2374|\n",
      "|2015-03-16 04:55:...|desktop|    2211|\n",
      "|2015-03-16 05:20:...|desktop|    2238|\n",
      "|2015-03-16 06:10:...|desktop|    2295|\n",
      "|2015-03-16 07:01:...|desktop|    2211|\n",
      "|2015-03-16 07:47:...| mobile|     878|\n",
      "|2015-03-16 08:20:...|desktop|    2227|\n",
      "|2015-03-16 09:03:...| mobile|     839|\n",
      "|2015-03-16 09:42:...| mobile|     863|\n",
      "|2015-03-16 10:39:...|desktop|    2391|\n",
      "|2015-03-16 11:33:...| mobile|     981|\n",
      "|2015-03-16 12:07:...|desktop|    2588|\n",
      "|2015-03-16 12:36:...| mobile|    1037|\n",
      "|2015-03-16 13:39:...|desktop|    2943|\n",
      "|2015-03-16 14:17:...| mobile|    1282|\n",
      "+--------------------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timeit: [T](code: => T)T\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def timeit[T]( code: => T): T ={\n",
    "    val now1 = System.nanoTime\n",
    "    val ret: T = code\n",
    "    val ms1 = (System.nanoTime - now1) / 1000000\n",
    "    println(\"Elapsed time: %d ms\".format( ms1))\n",
    "    ret\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7199995\n",
      "Elapsed time: 3554 ms\n"
     ]
    }
   ],
   "source": [
    "timeit({println(df.count)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache the data by creating a temporary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.createTempView(\"pageviews_by_second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.sqlContext.cacheTable(\"pageviews_by_second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pageDF: org.apache.spark.sql.DataFrame = [timestamp: timestamp, site: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val pageDF = spark.read.table(\"pageviews_by_second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7199995\n",
      "Elapsed time: 4129 ms\n"
     ]
    }
   ],
   "source": [
    "timeit({ println(pageDF.count) })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartition the data to improve parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the Event Timeline for this Job in the Spark UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Int = 5\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.SaveMode\n",
    "\n",
    "df.repartition(4)\n",
    "    .write.format(\"parquet\")\n",
    "    .mode(SaveMode.Overwrite)\n",
    "    .option(\"overwrite\", true)\n",
    "    .save(\"/Users/josiahsams/SparkDemo/datasets/repart-pageviews-by-second-tsv.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pageviewsDF: org.apache.spark.sql.DataFrame = [timestamp: timestamp, site: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val pageviewsDF = spark.read.parquet(\"/Users/josiahsams/SparkDemo/datasets/repart-pageviews-by-second-tsv.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7199995\n",
      "Elapsed time: 2174 ms\n"
     ]
    }
   ],
   "source": [
    "timeit({println(pageviewsDF.count)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[image1]: ./images/4_partitions_dashed.png\n",
    "![4 partitions][image1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cachedDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [timestamp: timestamp, site: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val cachedDF = pageviewsDF.orderBy(column(\"timestamp\"), column(\"site\").desc).cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Long = 7199995\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cachedDF.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: Int = 200\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cachedDF.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching with Caution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the partitions right after the cache call on the dataframe in Spark UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: cachedDF.type = [timestamp: timestamp, site: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cachedDF.unpersist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: String = 200\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sqlContext.getConf(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.sqlContext.setConf(\"spark.sql.shuffle.partitions\", \"4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res14: String = 4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sqlContext.getConf(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newcachedDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [timestamp: timestamp, site: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val newcachedDF = pageviewsDF.orderBy(column(\"timestamp\"), column(\"site\").desc).cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Long = 7199995\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "newcachedDF.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class pageview\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class pageview(timestamp: Option[java.sql.Timestamp], site: Option[String], requests: Option[Int])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sc1: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@21f1ed30\n",
       "dataset: org.apache.spark.sql.Dataset[pageview] = [timestamp: timestamp, site: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val sc1 = SparkSession.builder.getOrCreate()\n",
    "import sc1.implicits._\n",
    "val dataset = newcachedDF.as[pageview]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filterMapDS: org.apache.spark.sql.Dataset[pageview] = [timestamp: timestamp, site: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val filterMapDS = dataset.filter(_.site.getOrElse(\"\") == \"mobile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pageview(Some(2015-03-16 00:00:00.0),Some(mobile),Some(1628))\n",
      "pageview(Some(2015-03-16 00:00:01.0),Some(mobile),Some(1636))\n",
      "pageview(Some(2015-03-16 00:00:02.0),Some(mobile),Some(1619))\n",
      "pageview(Some(2015-03-16 00:00:03.0),Some(mobile),Some(1776))\n",
      "pageview(Some(2015-03-16 00:00:04.0),Some(mobile),Some(1716))\n",
      "pageview(Some(2015-03-16 00:00:05.0),Some(mobile),Some(1721))\n",
      "pageview(Some(2015-03-16 00:00:06.0),Some(mobile),Some(1695))\n",
      "pageview(Some(2015-03-16 00:00:07.0),Some(mobile),Some(1630))\n",
      "pageview(Some(2015-03-16 00:00:08.0),Some(mobile),Some(1731))\n",
      "pageview(Some(2015-03-16 00:00:09.0),Some(mobile),Some(1664))\n"
     ]
    }
   ],
   "source": [
    "filterMapDS.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs: Job, Stages, Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[image2]: ./images/picture1.png\n",
    "![stages][image2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[image3]: ./images/picture2.png\n",
    "![stages][image3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catalyst Optimizer: Logical Plan to Physical Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filterDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [timestamp: timestamp, site: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val filterDF = newcachedDF.filter(column(\"site\") === \"mobile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =\n",
       "'Filter ('site = mobile)\n",
       "+- Sort [timestamp#124 ASC, site#125 DESC], true\n",
       "   +- Relation[timestamp#124,site#125,requests#126] parquet\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filterDF.queryExecution.logical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[image4]: ./images/filter_count_run.png\n",
    "![logical plan][image4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('site = mobile)\n",
      "+- Sort [timestamp#0 ASC, site#1 DESC], true\n",
      "   +- Relation[timestamp#0,site#1,requests#2] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "timestamp: timestamp, site: string, requests: int\n",
      "Filter (site#1 = mobile)\n",
      "+- Sort [timestamp#0 ASC, site#1 DESC], true\n",
      "   +- Relation[timestamp#0,site#1,requests#2] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(site#1) && (site#1 = mobile))\n",
      "+- InMemoryRelation [timestamp#0, site#1, requests#2], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :  +- *Sort [timestamp#0 ASC, site#1 DESC], true, 0\n",
      "   :     +- Exchange rangepartitioning(timestamp#0 ASC, site#1 DESC, 4)\n",
      "   :        +- *BatchedScan parquet [timestamp#0,site#1,requests#2] Format: ParquetFormat, InputPaths: file:/Users/josiahsams/SparkDemo/datasets/repart-pageviews-by-second-tsv.parquet, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<timestamp:timestamp,site:string,requests:int>\n",
      "\n",
      "== Physical Plan ==\n",
      "*Filter (isnotnull(site#1) && (site#1 = mobile))\n",
      "+- InMemoryTableScan [timestamp#0, site#1, requests#2], [isnotnull(site#1), (site#1 = mobile)]\n",
      "   :  +- InMemoryRelation [timestamp#0, site#1, requests#2], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :     :  +- *Sort [timestamp#0 ASC, site#1 DESC], true, 0\n",
      "   :     :     +- Exchange rangepartitioning(timestamp#0 ASC, site#1 DESC, 4)\n",
      "   :     :        +- *BatchedScan parquet [timestamp#0,site#1,requests#2] Format: ParquetFormat, InputPaths: file:/Users/josiahsams/SparkDemo/datasets/repart-pageviews-by-second-tsv.parquet, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<timestamp:timestamp,site:string,requests:int>\n"
     ]
    }
   ],
   "source": [
    "filterDF.explain(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[image5]: ./images/catalyst.png\n",
    "![catalyst][image5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: Long = 3599997\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filterDF.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[image6]: ./images/filter_physical_model.png\n",
    "![filter_physical_model][image6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 WholeStageCodegen subtrees.\n",
      "== Subtree 1 / 1 ==\n",
      "*Filter (isnotnull(site#125) && (site#125 = mobile))\n",
      "+- InMemoryTableScan [timestamp#124, site#125, requests#126], [isnotnull(site#125), (site#125 = mobile)]\n",
      "   :  +- InMemoryRelation [timestamp#124, site#125, requests#126], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :     :  +- *Sort [timestamp#124 ASC, site#125 DESC], true, 0\n",
      "   :     :     +- Exchange rangepartitioning(timestamp#124 ASC, site#125 DESC, 4)\n",
      "   :     :        +- *BatchedScan parquet [timestamp#124,site#125,requests#126] Format: ParquetFormat, InputPaths: file:/Users/josiahsams/SparkDemo/datasets/repart-pageviews-by-second-tsv.parquet, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<timestamp:timestamp,site:string,requests:int>\n",
      "\n",
      "Generated code:\n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIterator(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ final class GeneratedIterator extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 006 */   private Object[] references;\n",
      "/* 007 */   private scala.collection.Iterator inputadapter_input;\n",
      "/* 008 */   private org.apache.spark.sql.execution.metric.SQLMetric filter_numOutputRows;\n",
      "/* 009 */   private UnsafeRow filter_result;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder filter_holder;\n",
      "/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter filter_rowWriter;\n",
      "/* 012 */\n",
      "/* 013 */   public GeneratedIterator(Object[] references) {\n",
      "/* 014 */     this.references = references;\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void init(int index, scala.collection.Iterator inputs[]) {\n",
      "/* 018 */     partitionIndex = index;\n",
      "/* 019 */     inputadapter_input = inputs[0];\n",
      "/* 020 */     this.filter_numOutputRows = (org.apache.spark.sql.execution.metric.SQLMetric) references[0];\n",
      "/* 021 */     filter_result = new UnsafeRow(3);\n",
      "/* 022 */     this.filter_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(filter_result, 32);\n",
      "/* 023 */     this.filter_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(filter_holder, 3);\n",
      "/* 024 */   }\n",
      "/* 025 */\n",
      "/* 026 */   protected void processNext() throws java.io.IOException {\n",
      "/* 027 */     while (inputadapter_input.hasNext()) {\n",
      "/* 028 */       InternalRow inputadapter_row = (InternalRow) inputadapter_input.next();\n",
      "/* 029 */       boolean inputadapter_isNull1 = inputadapter_row.isNullAt(1);\n",
      "/* 030 */       UTF8String inputadapter_value1 = inputadapter_isNull1 ? null : (inputadapter_row.getUTF8String(1));\n",
      "/* 031 */\n",
      "/* 032 */       if (!(!(inputadapter_isNull1))) continue;\n",
      "/* 033 */\n",
      "/* 034 */       boolean filter_isNull2 = false;\n",
      "/* 035 */\n",
      "/* 036 */       Object filter_obj = ((Expression) references[1]).eval(null);\n",
      "/* 037 */       UTF8String filter_value4 = (UTF8String) filter_obj;\n",
      "/* 038 */       boolean filter_value2 = false;\n",
      "/* 039 */       filter_value2 = inputadapter_value1.equals(filter_value4);\n",
      "/* 040 */       if (!filter_value2) continue;\n",
      "/* 041 */\n",
      "/* 042 */       filter_numOutputRows.add(1);\n",
      "/* 043 */\n",
      "/* 044 */       boolean inputadapter_isNull = inputadapter_row.isNullAt(0);\n",
      "/* 045 */       long inputadapter_value = inputadapter_isNull ? -1L : (inputadapter_row.getLong(0));\n",
      "/* 046 */       boolean inputadapter_isNull2 = inputadapter_row.isNullAt(2);\n",
      "/* 047 */       int inputadapter_value2 = inputadapter_isNull2 ? -1 : (inputadapter_row.getInt(2));\n",
      "/* 048 */       filter_holder.reset();\n",
      "/* 049 */\n",
      "/* 050 */       filter_rowWriter.zeroOutNullBytes();\n",
      "/* 051 */\n",
      "/* 052 */       if (inputadapter_isNull) {\n",
      "/* 053 */         filter_rowWriter.setNullAt(0);\n",
      "/* 054 */       } else {\n",
      "/* 055 */         filter_rowWriter.write(0, inputadapter_value);\n",
      "/* 056 */       }\n",
      "/* 057 */\n",
      "/* 058 */       filter_rowWriter.write(1, inputadapter_value1);\n",
      "/* 059 */\n",
      "/* 060 */       if (inputadapter_isNull2) {\n",
      "/* 061 */         filter_rowWriter.setNullAt(2);\n",
      "/* 062 */       } else {\n",
      "/* 063 */         filter_rowWriter.write(2, inputadapter_value2);\n",
      "/* 064 */       }\n",
      "/* 065 */       filter_result.setTotalSize(filter_holder.totalSize());\n",
      "/* 066 */       append(filter_result);\n",
      "/* 067 */       if (shouldStop()) return;\n",
      "/* 068 */     }\n",
      "/* 069 */   }\n",
      "/* 070 */ }\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filterDF.queryExecution.debug.codegen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[image8]: ./images/pipelining.png\n",
    "![pipeline][image8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Reference\n",
    "\n",
    "\n",
    "* Deep Dive into Project Tungsten: Bringing Spark Closer to Bare Metal\n",
    "\n",
    "    http://tinyurl.com/project-tungsten\n",
    "\n",
    "    https://www.youtube.com/watch?v=5ajs8EIPWGI\n",
    "    \n",
    "\n",
    "* Spark Performance: Whatâ€™s Next\n",
    "\n",
    "    https://www.youtube.com/watch?v=JX0CdOTWYX4\n",
    "    \n",
    "\n",
    "* Unified Memory Management\n",
    "\n",
    "    https://issues.apache.org/jira/browse/SPARK-10000\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
